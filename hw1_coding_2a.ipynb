{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Frm06ZPq8noY"
   },
   "source": [
    "# CMU auto-graded notebook\n",
    "\n",
    "Before you turn these assignments in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE`, `<FILL IN>`, or \"YOUR ANSWER HERE.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfii2P5r8nob"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1tnkgBl8nob"
   },
   "source": [
    "# CMU Machine Learning with Large Datasets\n",
    "\n",
    "## Homework 1 - Coding 2: Streaming Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxsV3uA48noc"
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment?\n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5ux33if8noc"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLWy2qaP8nod"
   },
   "source": [
    "### (1a) Environment Setup\n",
    "\n",
    "Run the following cell to establish the runtime environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq_Yf3an8nod"
   },
   "outputs": [],
   "source": [
    "# Ignore this cell if the environment already has the packages\n",
    "\n",
    "%pip install nose numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yl9JFmyO8nod"
   },
   "outputs": [],
   "source": [
    "# imports that will be used in the notebook -- shouldn't need to import any other libraries\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV7vgXw08noe"
   },
   "source": [
    "### (1b) Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k03fgbvg8noe"
   },
   "source": [
    "We use the Reuters Corpus (RCV1), which is a set of news stories split into a hierarchy of categories. There are three file sets. The two data sets with \"small\" in the name contain smaller subsamples of the full data set. They are provided for debugging and local tests. The final classification task should use the \"full\" one. Each data set is split into a train and test set, as indicated by the file suffix:\n",
    "\n",
    "- RCV1.full_train.txt\n",
    "- RCV1.full_test.txt\n",
    "- RCV1.small_train.txt\n",
    "- RCV1.small_test.txt\n",
    "- RCV1.very_small_train.txt\n",
    "- RCV1.very_small_test.txt\n",
    "\n",
    "Download the data using the link provided in the handout, and fill in the corresponding variables with their file paths in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if running on Databricks\n",
    "\n",
    "# from urllib.parse import urlparse\n",
    "# from pyspark import SparkFiles\n",
    "\n",
    "# def get_file(url):\n",
    "#     parsed_url = urlparse(url)\n",
    "#     suffix = parsed_url.path.split('/')[-1]\n",
    "#     sc.addFile(url)\n",
    "#     path = SparkFiles.get(suffix)\n",
    "\n",
    "#     return path\n",
    "\n",
    "# FULL_TRAIN = get_file(\"https://cmu.box.com/shared/static/g37w2q552qaik60yubrd3aq95szgi7pc\")\n",
    "# FULL_TEST = get_file(\"https://cmu.box.com/shared/static/305174svzxe3lqm0ps76fvaenhr37inj\")\n",
    "# SMALL_TRAIN = get_file(\"https://cmu.box.com/shared/static/f1x91jode5ywofu5b6cm9tb6ynx3yoft\")\n",
    "# SMALL_TEST = get_file(\"https://cmu.box.com/shared/static/0l8y7x3gtix2sjfb1ewzq10c8ayaokdd\")\n",
    "# VERY_SMALL_TRAIN = get_file(\"https://cmu.box.com/shared/static/xiumqmfdge5muxhs9v2w7ma97hurqtqy\")\n",
    "# VERY_SMALL_TEST = get_file(\"https://cmu.box.com/shared/static/np2s7tn5wvwuawad2vddygs76tbwt1f5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yyo_7ru08noe"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate file paths\n",
    "FULL_TRAIN = <FILL IN>\n",
    "FULL_TEST = <FILL IN>\n",
    "SMALL_TRAIN = <FILL IN>\n",
    "SMALL_TEST = <FILL IN>\n",
    "VERY_SMALL_TRAIN = <FILL IN>\n",
    "VERY_SMALL_TEST = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZbUFSjw8noe"
   },
   "source": [
    "There are multiple class labels per document, meaning that there is more than one correct answer to the question \"What kind of news article is this?\"\n",
    "\n",
    "For this assignment, we will ignore all class labels except for those ending in CAT and just be classifying into the top-level nodes of the hierarchy:\n",
    "\n",
    "- CCAT: Corporate/Industrial\n",
    "- ECAT: Economics\n",
    "- GCAT: Government/Social\n",
    "- MCAT: Markets\n",
    "\n",
    "DO NOT change the following cell and just run it to set up the CAT labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qKcB8ML8nof"
   },
   "outputs": [],
   "source": [
    "CAT_LABELS = ['CCAT', 'ECAT', 'GCAT', 'MCAT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scv5Rgiz8nof"
   },
   "source": [
    "The data format is one document per line, with the class label(s) first (comma separated), a tab character, and then the document text.\n",
    "\n",
    "Run the following cell to take a glance at the first document of the small training data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0CVLqKZ8nof"
   },
   "outputs": [],
   "source": [
    "with open(SMALL_TRAIN, 'r') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKKg-Q88nof"
   },
   "source": [
    "### (1c) Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzF7Uocx8nof"
   },
   "source": [
    "To count the words, we need to tokenize the document text. In real-world practice, this may involve multiple steps, such as removing stop words and converting text to lowercase, which you learned in HW1: Entity Resolution. For this Naive Bayes part, we simplify the process by splitting only on words.\n",
    "\n",
    "Run the following cell to define the `tokenization(doc)` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fp01Z4-K8nof"
   },
   "outputs": [],
   "source": [
    "# DO NOT change this function\n",
    "def tokenizeDoc(doc: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Convert input document text into tokenization features\n",
    "    Args:\n",
    "        doc: document text\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    return re.findall('\\\\w+', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5s0UoRA8nof"
   },
   "source": [
    "As the handout instructs, streaming Naive Bayes loads one-line document at a time, use that document to update the classifier statistics, and then discard the document. After loading a line, we need a function to parse the line for classifier training.\n",
    "\n",
    "Implement `parseDatafileLine(datafileLine)` that takes a line (labels + document text) and return a list of labels and a list of tokens. You need to use the `tokenizeDoc(doc)` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kuQlvTC8nof"
   },
   "outputs": [],
   "source": [
    "def parseDatafileLine(datafileLine: str):\n",
    "    \"\"\"\n",
    "    Parse a line of the data file to separate labels and document tokens\n",
    "    Args:\n",
    "        datafileLine: input string that is a line from the data file\n",
    "    Returns:\n",
    "        labels (list), tokens (list)\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FFa2bdI8nof"
   },
   "outputs": [],
   "source": [
    "\"\"\"Test parseDatafileLine(datafileLine)\"\"\"\n",
    "\n",
    "line_sample1 = \"C15,C151,CCAT\\tMcDonald's Corp said Thursday it raised its quarterly dividend 10 percent, to $0.0825 a share from $0.075.\"\n",
    "line_sample2 = \"C15,C151,CCAT\\tSix months to Sept 30, 1996       (in million rupees unless stated)\"\n",
    "\n",
    "assert_equal(parseDatafileLine(line_sample1),\n",
    "             (['C15', 'C151', 'CCAT'],\n",
    "              ['McDonald', 's', 'Corp', 'said', 'Thursday', 'it', 'raised', 'its',\n",
    "               'quarterly', 'dividend', '10', 'percent', 'to', '0', '0825', 'a',\n",
    "               'share', 'from', '0', '075']))\n",
    "\n",
    "assert_equal(parseDatafileLine(line_sample2),\n",
    "             (['C15', 'C151', 'CCAT'],\n",
    "              ['Six', 'months', 'to', 'Sept', '30', '1996', 'in', 'million',\n",
    "               'rupees', 'unless', 'stated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjYi1Fca8nog"
   },
   "source": [
    "### 1(d) Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_zH3_lG8nog"
   },
   "source": [
    "Now, we are good for training. We will use a dictionary as the model to store the count statistics.\n",
    "\n",
    "As the handout instructs, the model contains five parts:\n",
    "\n",
    "- `y`: $(Y=y)$ for each label y the number of training instances of that class\n",
    "- `ys`: $(Y=*)$ here $*$ means anything, so this is just the total number of training instances\n",
    "- `y_w`: $(Y=y,W=w)$ number of times token w appears in a document with label y\n",
    "- `y_ws`: $(Y=y,W=*)$ total number of tokens for documents with label y\n",
    "- `vocabulary`: track the vocabulary for Laplace smoothing\n",
    "\n",
    "Recall that Laplace smoothing formula is\n",
    "\n",
    "$$p(W=w_i|Y=y)=\\frac{count(Y=y,W=w_i) + \\alpha}{count(Y=y,W=*)+ \\alpha|V|}$$\n",
    "where $|V|$ is the vocabulary.\n",
    "\n",
    "Implement `nbmodel()` by figuring out the proper variable types for each part and filling in the blank below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMc-kFwL8nog"
   },
   "outputs": [],
   "source": [
    "def nbmodel() -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict storing the required five parts\n",
    "    \"\"\"\n",
    "    # TODO: Replace <FILL IN> with appropriate code\n",
    "    return {\n",
    "        'y': <FILL IN>,\n",
    "        'ys': <FILL IN>,\n",
    "        'y_w': <FILL IN>,\n",
    "        'y_ws': <FILL IN>,\n",
    "        'vocabulary': <FILL IN>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BUeeEaf8nog"
   },
   "source": [
    "Implement `trainNB(trainfile, model)` that loads one document at a time and uses that document to update the required statistics for the Naive Bayes classifier.\n",
    "\n",
    "Hint:\n",
    "\n",
    "1. We only use those lables ending in CAT, which defined in `CAT_LABELS` earlier. Therefore, you need to figure out a way to skip other labels.\n",
    "2. There are some documents with more than one CAT label. Treat those documents as multiple data instances (that is, add to the counters for\n",
    "   all labels ending in CAT). For instance, if one line contains CCAT and ECAT, use the same document text twice.\n",
    "3. It is not necessary to return the model here if you use proper types. Think about Python's mutable vs immutable types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnuvT2vN8nog"
   },
   "outputs": [],
   "source": [
    "def trainNB(trainfile: str, model: dict):\n",
    "    \"\"\"\n",
    "    Update the model in a streaming fashion.\n",
    "    Args:\n",
    "        trainfile: file path of the training data\n",
    "        model: dict of the Naive Bayes classifier model\n",
    "    \"\"\"\n",
    "    with open(trainfile, 'r') as f:\n",
    "        # Please note here we use f for iteration directly instead of f.readlines().\n",
    "        # Think about the reasons from the perspective of streaming\n",
    "        for line in f:\n",
    "            labels, tokens = parseDatafileLine(line)\n",
    "\n",
    "            # TODO: YOUR CODE HERE\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJwHoz4m8nog"
   },
   "source": [
    "### 1(e) Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N_CFBED8nog"
   },
   "source": [
    "For each line of documents, your classification code should get the best class $y$ and its log probabilities:\n",
    "\n",
    "$$ln(p(Y=y))+\\sum_{w_i} ln(p(W=w_i|Y=y))$$\n",
    "\n",
    "Notice that we use the natural logarithm here.\n",
    "\n",
    "Implement `testNB(testfile, model)` that uses the trained model to classify the test data and return a list of best classes, a list of max log probabilities (**rounding it to 4 decimal places**), and overall accuracy (**rounding it to 4 decimal places**). Please explicitly return in this specified order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gWRRfMu8nog"
   },
   "outputs": [],
   "source": [
    "def testNB(testfile, model):\n",
    "    \"\"\"\n",
    "    Implement Naive Bayes classification\n",
    "    Args:\n",
    "        testfile: file path of the test data\n",
    "        model: dict of the Naive Bayes classifier model\n",
    "    Returns:\n",
    "        best_classes, log_probabilities, accuracy\n",
    "    \"\"\"\n",
    "    best_classes = <FILL IN>\n",
    "    log_probabilities = <FILL IN>\n",
    "\n",
    "    with open(testfile, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            labels, tokens = parseDatafileLine(line)\n",
    "\n",
    "            # TODO: YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    return best_classes, log_probabilities, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XodcTc5w8nog"
   },
   "outputs": [],
   "source": [
    "\"\"\"DO NOT change this this cell and just run it to use the very small dataset to test your implementations\"\"\"\n",
    "\n",
    "very_small_model = nbmodel()\n",
    "trainNB(VERY_SMALL_TRAIN, very_small_model)\n",
    "best_classes, log_probabilities, accuracy = testNB(VERY_SMALL_TEST, very_small_model)\n",
    "\n",
    "assert_equal(best_classes,\n",
    "             ['MCAT', 'ECAT', 'CCAT', 'ECAT', 'CCAT', 'CCAT', 'ECAT', 'CCAT'])\n",
    "assert_equal(log_probabilities,\n",
    "             [-9893.7804, -3912.8180, -1121.5992, -1610.1660,\n",
    "              -701.3466, -1453.3430, -2218.3302, -2285.0698])\n",
    "assert_equal(accuracy, 0.8750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8Stku3d8nog"
   },
   "source": [
    "### 1(f) Full Classification and Deliverable\n",
    "\n",
    "We are almost there! Let's wrap up this assignment.\n",
    "\n",
    "Implement your training and test functions on the full dataset to get the full classification results. Please note that you need to define a new model different from the `very_small_model` we have already tested.\n",
    "\n",
    "Write the full classification results to a file `full_result.txt` (please explicitly use this name). The output format should have one test result per line, and each line should have the format:\n",
    "\n",
    "$$\\text{[Label1, Label2, ...]<tab>Best class<tab>Log prob}$$\n",
    "\n",
    "where **[Label1, Label2, ...]** are the true labels of the test instance, **Best class** is the class with the maximum log probability, and the last field is the log probability.\n",
    "\n",
    "The last line of the file should include the accuracy.\n",
    "\n",
    "Use the following cell to write your code.\n",
    "\n",
    "Submit both this notebook and `full_result.txt` to Gradescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzdWLHm58nog"
   },
   "outputs": [],
   "source": [
    "# * Here is an expected output of very_small_test dataset for your reference\n",
    "# * You need to write one using the FULL dataset\n",
    "\n",
    "'''\n",
    "['C24', 'CCAT', 'M14', 'MCAT']\\tMCAT\\t-9893.7804\n",
    "['E51', 'E512', 'ECAT', 'GCAT', 'GDIP']\\tECAT\\t-3912.8180\n",
    "['C15', 'C152', 'C18', 'C181', 'CCAT']\\tCCAT\\t-1121.5992\n",
    "['GCAT']\\tECAT\\t-1610.1660\n",
    "['C13', 'CCAT', 'GCAT', 'GHEA']\\tCCAT\\t-701.3466\n",
    "['C13', 'CCAT', 'M11', 'MCAT']\\tCCAT\\t-1453.3430\n",
    "['C11', 'C13', 'CCAT', 'E12', 'ECAT', 'M13', 'M132', 'MCAT']\\tECAT\\t-2218.3302\n",
    "['C31', 'CCAT']\\tCCAT\\t-2285.0698\n",
    "Accuracy: 7/8=0.8750\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "10605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
